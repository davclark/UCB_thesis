\graphicspath{{appendices/kappa-extension/}}

% Note - pdf doesn't support extended characters in TOC
\chapter{Extension to Cohen's \texorpdfstring{$\kappa$}{kappa} for multiple scores}
\label{app:kappa}

$\kappa$ is a commonly used measure of agreement developed by Cohen in which
different raters each apply one of a number of scores to a set of responses
(usually written text, such as collected in the studies in
Chapter~\ref{chap:mechanism}).
% TODO: Include standard formulation and reference.
Our texts recieve a number of codes, so we needed an extension to $\kappa$ that
could capture such a scheme. We start from the idea that if both coders provide
the same set of codes, they should be given a 1, or credit for full agreement.
If they provide no common codes, they recieve a 0. Partial credit when both
coders provide the same number of codes is equally simple. Consider the case
where coders provide $N$ codes, $m$ of which are common. This item then recieves
a score of $m/N$. In the case of an unequal number of codes, $N$ is set to the
(potentially non-integer) average of the two numbers. Given these scores for
each item, $\kappa$ is computed normally as the ratio between observed and
chance agreement. 
% TODO: Make this more correct.

A note on using $\kappa$ in the text above: it seems completely admissable to
continue to use $\kappa$ for the measure described above, given it's similarity
in spirit to Cohen's original measure. This seems doubly admissable considering
that formal distributional considerations appear to be considered rarely, if
ever, in the psychological literature. Thus, any deviations in the specifics of
formal properties are unlikely to be a concern for the casual reader.
