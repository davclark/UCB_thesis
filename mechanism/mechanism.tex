\graphicspath{{mechanism/}}

\chapter{Teaching the Mechanism of the Greenhouse Effect}
\label{chap:mechanism}

As described in Chapters~\ref{chap:intro} and \ref{chap:survey}, American's lag
much of the world regarding acceptance of anthropogenic (i.e., “human caused”)
climate change. Informally, Michael Ranney, then other members of our group
started questioning whether people were able to mechanistically explain how
human activities cause an increase in global mean temperature.  Almost no one
could provide a satisfactory explanation, including most of us! As described in
Section~\ref{sec:naive}, no surveyed members of the general public were able to
offer a credible description, but the quality of partially correct responses
correlated significantly with GW attitudes. 

Our investigations revealed that almost no-one---only a few academic
experts and \emph{no} members of the general public---knew the basic
concepts described in our 400 words. Moreover, variation along measured
knowledge was correlated with individuals' attitudes. Thus, we developed and evaluated an
science education intervention (arguments for this approach are also provided in
Section~\ref{sec:science-ed}). Prof. Ranney, Lloyd Goldwasser, and Daniel
Reinholz (along with input from myself and Ronald Cohen) first developed a
short \~400-word description of the mechanism. This text is reproduced in full
in Appendix~\ref{app:400words} (and in a more condensed form in
\cite{ranney_improving_2012_f}).  Before reading the text yourself, I would
encourage you to spend 10 minutes describing \emph{your} understanding either
aloud or on paper.\footnote{If you personally doubt the veracity of
    anthropogenic climate change, then you may modify the exercise to describing
    the mechanism of the greenhouse effect, first described by Nobel laureate
    Svante Arrhenius in \citeyear{arrhenius_influence_1896}.}

 In these experiments, we sought to formally ask:
\begin{enumerate}
\item Is this lack of understanding for the mechanism of global climate change
    as pervasive as it seemed to be?
\item Does instruction regarding the mechanism of global climate change increase
    individuals' acceptance of the reality of anthropogenic climate change?
\end{enumerate}
Along the way, we additionally considered related aspects of learners'
cognition (the details of which are described below).

The history of educational research would imply that it’s quite difficult to
arrive at definitive answers to big policy questions. For example, phonics vs.
whole-word reading has been debated at least since the dawn of the Common Era,
as discussed in \textcite{compayre_history_1889}. Below, however, I report
on a series of experiments that argue strongly (if not definitively!) that
instruction regarding the physical mechanism of the greenhouse effect appears to
have some positive effect on public acceptance of anthropogenic climate change.
As discussed above, such public acceptance seems central to any truly democratic
approach to the problem of climate change.

As discussed in Chapter~\ref{chap:intro}, others’ studies detract from the
utility of approaching climate change as a science education problem
\parencite[e.g.,]{lord_biased_1979,kahan_polarizing_2012}. In that discussion,
we noted numerous potential shortcomings of such studies, such as the exclusion
of participants with moderate attitudes. In this intervention, we focus on
tackling one notable shortcoming. Specifically, the interventions in this chapter 
focus on a fundamental, well-researched knowledge gap, and our assessment
focuses on acceptance/belief.  Such contrasts may explain the difference between
observing instructional benefits (as we have) or polarization (as others
occasionally have; \cite[cf.]{lundmark_new_2007}).  We'll see further evidence
below, however, that such interventions are applicable across a variety of
settings, time-frames, and populations, and that global warming understandings
and attitudes are far from static. Most importantly, such understandings seem to
affect attitudes and beliefs in a meaningful way.


\section{Study 1: Lecture Room Interventions at UC and UT}
\label{sec:mech-classroom}

In this chapter, we are considering the efficacy of a mechanistic explanation of
climate change in addressing the educational challenges laid out in
Chapters~\ref{chap:evilndi} and \ref{chap:prondi}. It should be noted, however,
that this experiment was actually carried out prior to the those experiments.
Thus, this experiment was a particularly thick, exploratory observation of
individuals' beliefs, attitudes and knowledge. Here, “thick” means that we
explored the same phenomenon through multiple routes---for example, doing keyword
searches of textual responses, and examining coded responses though a number of
metrics. 

The methods are largely analogous to those in the previous 2 chapters (see in
particular Section~\ref{sec:evilndi-methods}). Here, as opposed to a primarily
numerical intervention, we sought to understand how a
relatively brief 400-word mechanistic explanation, including two of the more
surprising numbers from Chapter~\ref{chap:prondi}, might affect climate-relevant
beliefs and concern,
as well as how this might be modulated by prior commitment to one's own
explanations and stated attitudes.  The general flow of the experiment is given
in Figure~\ref{fig:mech-flow}.

\begin{figure}[h]
    \includegraphics[width=6.5in]{mech-survey-flow1.pdf}
    \caption{An overview of the experimental flow for
        Section~\ref{sec:mech-classroom}. The flow for other experiments in
        Chapter~\ref{chap:mechanism} was similar. The analogy to a sandwich
        takes the knowledge and attitude tests to be slices of bread, and the
        educational intervention itself is the “jam.”}
    \label{fig:mech-flow}
\end{figure}

The primary goal here was a proof of concept. By assessing university
students---some of our nation’s most highly educated citizens---we provide a
strong test of our hypothesis that most Americans are ignorant of the mechanism
of global climate change. An additional concern was that for maximal power, it
is preferable to sample na\"ive beliefs prior to the intervention. In such a
design, we are able to use repeated measures statistics and consequently have
much greater power. On the downside, however, problems can arise from an
assessment prior to an intervention. For example, we were concerned that
individuals might exhibit an increase in their stated belief in anthropogenic
climate change merely by dint of experimental demand. This is evaluated by
comparing our sandwich and no-pretest groups.  As described in
Section~\ref{chap:survey}, pretest responses can be used to asses na\"ive
knowledge and attitudes in the general public. 

% Ryan thinks this is unconvincing, game-show like. Could make a parallel with
% Jame's psychophysical experiments, where people could clearly know that an
% apple is red, but they need to be trained to report the basic visual
% properties of what they see preferentially over reporting "basic level"
% phenomena. The "basic pieces" of our mechanism do indeed seem new to most, but
% we have yet to formally test that part.

\subsection{Methods}

\subsubsection{Participants}

One-hundred three University of California, Berkeley, and 46 University of
Texas, Brownsville, undergraduates were randomly assigned to one of our two
groups: “sandwich” or “no-pretest.” UT Brownsville is an “Hispanic-Serving
Institution” with over 96\% of the student body reporting as Hispanic. UC
Berkeley is somewhat more racially diverse, with no overarching mission in that
respect. Students were recruited from large lecture courses. Those from
Brownsville were recruited from courses in the physical sciences, while Berkeley
participants were recruited from a course in cognitive science.  Below, we
report data from the 85 Berkeley and 41 Brownsville students who completed the
survey as intended and had been U.S. residents for ten years or more (because we
expressly consider U.S. exceptionalism/nationalism). Of the Berkeley data, we
analyzed 43 no-pretest surveys and the pretest part of 42 sandwich surveys---but
due to anticipated time constraints, only 30 sandwich posttests could be
completed/obtained. Of the Brownsville data, we analyzed 22 complete no-pretest
and 19 complete sandwich surveys.

Of the 85 Berkeley students analyzed, two did not complete the demographic test.
Forty-three were female and 40 were male. Mean conservatism was 3.69 (i.e., slightly
liberal on our 9-point scale; 1.65 standard deviation). Of the 41 Brownsville
students 21 were female, 20 were male. Here, mean conservativism was 4.95 (i.e.,
moderate; 1.77 $sd$). Political affiliation is reported in
Table~\ref{table:study1-affiliation}.

\begin{table}
    \centering
    \caption{Number of students from both sub-populations stating membership in
        a given political party. Note that the demographic survey was given
        after the intervention (and thus may have influenced participants'
        willingness to state their political affiliation).}
    \label{table:study1-affiliation}
    \begin{tabular}{rcc}
        \toprule
        Party & Berkeley & Brownsville \\
        \midrule
          decline to state &   5  & 7 \\
          democrat &  40  & 7 \\
          independent &   4  & 2 \\
          libertarian &   9  & 1 \\
          none &  24   & 16 \\
          other &   1  & 1 \\
          republican &   2  & 3 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Materials and Procedure}

The general flow of the intervention is given in Figure~\ref{fig:mech-flow}, and
was collected anonymously.  A sample of the format of the core no-pretest
intervention is given in Appendix~\ref{app:mech-format}. Participants were split
into two groups, receiving either the “no-pretest” version of the intervention
(sometimes called “open-faced”), or the full “sandwich” (filled with nourishing
descriptions of climate change!). 

The climate change knowledge portion of the pre and posttests consisted of the
three questions described in the Appendix, Section~\ref{sec:materials}. For this
experiment, the Likert items (all on a 1-9 scale) consisted only of
\textsf{knwgbl} followed by the first 13 items in
Table~\ref{table:rtmd-questions} in the Appendix. Both groups read the
educational text regarding the mechanism of greenhouse gases (reproduced in
full in Appendix~\ref{app:400words}), and indicated any surprise they may have
experienced (again, on a 1-9 scale). The “kinds of light” check consisted of two
fill-in-the-blank questions regarding both the kinds of light coming to earth from
the sun and radiating away. Here, “sunlight” or “visible light” were considered
correct for incoming, and “infrared” was considered correct for outgoing.  Some
participants wrote “ultraviolet” for incoming light, which one could charitably
ascribe to a partially correct understanding.

After completing the intervention described above, participants also
completed a demographic survey, detailed in Table~\ref{table:demographics}.
The experiment began with a page of instructions, including the assertion that
no tricks or deceptions were involved in this study. Lastly, given that their
experimental intervention was shorter, individuals in the no-pretest group
were asked to provide some feedback on the intervention, and also on Al Gore’s
“An Inconvenient Truth” (if they had seen it). These responses were used to
refine our methods going forward, and \textcite{bem_feeling_2011} notwithstanding,
should have had no effect on the results.

Participants were run simultaneously for each of the two classes. Instructions
were administered by the course instructor, and students received one of two
packets---placing them into one of the two groups described above. After
completing the consent form on the front of the packet, individuals proceeded to
read and answer questions. The entire experiment required approximately
25 minutes to complete.

\subsubsection{Analysis}

Handwritten responses were coded and placed into a spreadsheet (for details, see
Appendix~\ref{app:coding}). Given the rich nature of these data, many analyses
were employed. As such, please see the Results section that follows for details
of the analysis used for each question.

\subsection{Results}

Please note that all statistical tests are reported in full in the tables
associated with this section. In the text, I primarily indicate only a basic
value to give a sense for the strength of the result.

\subsubsection{Scored Knowledge: Learning the Global Warming Mechanism}

Even our rather sophisticated samples initially exhibited incorrect or
non-normative understandings of the greenhouse effect’s mechanism (e.g., on the
roles of ultraviolet light, the ozone layer’s depletion, non-greenhouse-gas
pollution, and the reflection of incoming light). Most notably, not a single
pretest explanation mentioned different light/radiation types or atmospheric
retention time, despite an explicit prompt to explain any differences between
the energy traveling toward and away from Earth. However, after reading the
400-word description, 61\% of the Berkeley participants across both groups
correctly answered that “infrared” light was emitted from Earth (in its
fill-in-the-blank space), as did 55\% of the Brownsville students who responded.

Beyond the blank-filling items, we statistically analyzed individuals’
qualitative explanations—creating scoring rubrics for three central concepts:
\begin{description} \item[Light] Differentiating between the types of light
        entering and exiting the atmosphere \item[GHGs] Atmospheric greenhouse
        gases’ interactions with radiation \item[Energy] The increased
        atmospheric retention time of energy \end{description} Inter-rater
reliability was computed using a weighted modification of Cohen's $\kappa$,
described in full in Appendix~\ref{app:kappa}. This reliability was high
(weighted $κ = .71$ based on about one-third of the Berkeley data; $κ = .67$
across the full Brownsville dataset). Scores were generated based on three
separate aspects of understanding captured in the coded texts: “Light,” “GHGs,”
and “Energy.” Significant improvements were observed across all three subscores
($p < .05$ for all six improvement possibilities across the two conditions and
three subscore categories). We had no particular hypotheses, however, regarding
specific effects of a given concept. Therefore, the data reported below and in
Figure~\ref{fig:class-scored-knowledge} use combined knowledge scores (and each
sub-score contributes three of nine total possible points).

\begin{figure}
    \centering
    \includegraphics{class-scored-knowledge.pdf}
    \caption{Combined scored knowledge for participants in our lecture room
        interventions. Posttest scores for both groups improved significantly
        relative to sandwich pretest scores ($p < .01$)}
    \label{fig:class-scored-knowledge}
\end{figure}

% TODO: convert this statistic to one using our "good" dataset
Improvements in participant knowledge were readily obtained with different
approaches to analysis. For example, for our Berkeley students' responses, few
items received a “mechanistic” code on the pretest (including incorrect codes;
11/42), but the majority of responses received such a code on the posttest
(26/30 for the sandwich group, 39/43 for the no-pretest group).

\subsubsection{Self-Rated Knowledge}

Participants from both universities experienced significant gains in their self-rated
knowledge after the intervention as well ($p < .01$). However, for our
Brownsville students in the no-pretest group, they reported a posttest
self-rated knowledge rating almost numerically identical to pretest ratings in
the sandwich group. And while Berkeley students in the no-pretest group
increased significantly ($p < .05$), that increase was numerically smaller than the
sandwich group. These ratings are reported in
Figure~\ref{fig:class-self-rated}.

\begin{figure}
    \centering
    \includegraphics{class-self-rated.pdf}
    \caption{Gains in \emph{self-rated} knowledge scores for participants in our
        lecture room
        interventions. Gains in our sandwich groups were significant ($p
        < .01$), while gains for our no-pretest groups were non-existent
        in the Brownsville group, and smaller (but still significant; $p < .05$)
        in the Berkeley group.}
    \label{fig:class-self-rated}
\end{figure}

\subsubsection{Global Warming Acceptance Through Mechanistic Learning}

To arrive at an easily comparable measure of global warming acceptance, we
averaged together all of the items starting with “\textsf{gw}” used in this
study (See Appendix~\ref{app:survey-items} for a full list of these items).
The \textsf{lifsty} item was omitted due to some concerns regarding multiple
interpretation. This concern was in fact unfounded---this construct shifted
similarly to the others---but we retain this set of items throughout our
statistical testing to maintain consistency and genuine \emph{a priori}
hypothesis testing. 

It may seem quite remarkable, but participants’ global warming acceptance
increased dramatically after our brief intervention, as predicted.
Proportionally, participants shifted on average 14\% closer to ``extreme''
agreement with climate change items. To assess this, we used all of the 73
Berkeley posttest ratings in a paired $t$-test, and used imputation for pretest
scores for the no-pretest group. We found a significant change in global warming
acceptance on the posttests, as compared to pretest measures ($t(72) = 2.28$, $p
= .01$). This result was replicated with the Brownsville surveys ($t(39) =
4.24$, $p < .0001$). These ratings are given in
Figure~\ref{fig:class-gw-ratings}.

\begin{figure}
    \centering
    \includegraphics{class-gw-ratings.pdf}
    \caption{Changes in mean of Climate Change related beliefs and attitudes.
        Improvements from pre to posttest were significant for
        Berkeley students ($p < .05$ using a combined $t$-test with imputation).
        Improvements for Brownsville students were also significant using
        imputation ($p < .0001$), as well as looking only at the sandwich group
        ($p < .01$).}
    \label{fig:class-gw-ratings}
\end{figure}

\subsubsection{Predicting Na\"ive GW Beliefs and Attitudes}

The relationship between knowledge and attitudes was also reflected in Berkeley
students’ naïve pretest data, in which participants’ \emph{self-perceived} ratings of
their own global warming knowledge correlated significantly with their global
warming attitudes ($r = .39$, $p = .01$). This was not the case with
Brownsville students ($r = .15$, $p = .55$). This may be reflective of an overall
lower self-perceived knowledge by Brownsville students. But consider the
findings above, in which we see an even more striking difference in terms of
self-rated knowledge between our Berkeley and Brownsville populations. It seems
that Brownsville students may simply have a much less grounded notion of their
self-knowledge when they are not provided with any context on the matter. The
relationship between self-rated knowledge and GW attitudes is depicted in
Figure~\ref{fig:class-predicting-gw}.

\begin{figure}
    \centering
    \includegraphics{class-predicting-gw.pdf}
    \caption{Relationship between na\"ive pretest self-rated knowledge and mean
        GW beliefs and attitudes. These data were only available for
        participants who took the pretest (i.e., the “sandwich” group). A
        significant relationship obtains for the Berkeley students ($p < .01$),
        while there is little relationship in the Brownsville sample (i.e., the
        slope of the regression line above is much flatter). Note that no
        (significant) prediction of attitudes was possible based on scored
        knowledge.}
    \label{fig:class-predicting-gw}
\end{figure}

\subsubsection{Surprise}

Please recall that we had also predicted a between-conditions difference in
surprise ratings due to reduced hindsight biases among the sandwich
participants. The difference for Berkeley students was at the significance
border-line ($t(42.08) = 1.65$, $p = .05$). These surprise ratings increased
from a mean of 2.3 to 3.0 on a 9-point scale. It is a bit curious that
their ratings are so low in general! The surprise ratings only reached “6” in
the no-pretest condition (out of 9, with “5” being “somewhat surprising”), but
were as high as “9” (i.e., “extremely surprising”) in the sandwich condition.
This difference in distribution is depicted in
Figure~\ref{fig:uc-mech-surprise}.  Among Brownsville students, surprise was
uniformly higher, with a numerically similar difference between conditions,
although this result was not significant ($t(38.1) = 0.92$, $p = .18$). 

\begin{figure}[h]
    \centering
    \subfloat{\includegraphics[width=0.5\textwidth]{hypotheses-surprisedistributions1.pdf}}
    \subfloat{\includegraphics[width=0.5\textwidth]{hypotheses-surprisedistributions2.pdf}}
    \caption{Distributions of surprise ratings for the sandwich and open-faced
        conditions in the Berkeley sample. Note that for the sandwich condition,
        the slight increase in ``1'' ratings (which may indicate resistance to
        the intervention) co-occurs with an increase (from none) in ratings 7--9.}
    \label{fig:uc-mech-surprise}
\end{figure}

I suspect that it is unlikely that individuals experienced the same kind of
``visceral'' surprise from the blurb that can be obtained by, for example,
statistics we've used regarding issues like abortion and the death penalty.
Further, while it may be due to a limitation of imagination, I have difficulty
imagining an evolution item that would elicit that kind of surprise, either.

\subsection{Full Tables of Results}

As this was one of the very first experiments in this climate education program,
we explored a large number of hypotheses. While it would be burdensome to treat
all of these in the text, they are included here for posterity. Descriptions are
provided in table captions.

% In addition, we replicated the relationships predicted by the RTMD theory in
% another population. Confirmatory factor analysis yielded quite similar results
% to simple correlation tables between the means of attitude-relevant items. In
% particular, all proximal relationships held in this population (those
% represented by lines in Figure~\ref{fig:rtmd}) and in each survey, either 12
% or 13 out of 15 total relationships were in the direction predicted by RTMD.
% Less formally, it appears that the correlations between evolution and climate
% change increased after our intervention---perhaps indicating a shift in which
% participants viewed climate change as part of ``real science.'' Similar
% increases in anti-correlation with nationalism were observed. However, we have
% not yet established appropriate statistical machinery to test the significance
% of these effects.
 
% Currently, this is just significant results for the UC CogSci class (2010?)
% from “general improvements”
% copy pasted from mechanism/uc-classroom-summary.csv
% siunitx seems to be doing fine, but you can set it up like this if desired:
% \sisetup{round-precision=2,round-mode=figures,scientific-notation=true}
% I like table-parse-only. This doesn't work somehow: [table-number-alignment = center]
\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

% Final \\ necessary to compile!
\caption{Summary of “improvement” results for Berkeley lecture room interventions.
    All results were \emph{a priori} unless description starts with \emph{“post
        hoc”}.  \label{table:improvements-classroom}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Improvements in Berkeley lecture room interventions, continued}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Individuals rarely provided mechanistic responses on the pretest 
(11/42 responses received a mechanistic code), but they often do on the
posttest (26/30 sandwich and 39/43 no-pretest responses received a mechanistic
code on the posttest). The statistical test is computed only for the sandwich group pre vs.
posttest, which has a lesser prevalence of mechanistic responses on the posttest
of the two conditions &	3.2E-07	&
Fisher's exact test (two-sided)	\\
Misconceptions are common in the pretest but not the post test, total .38 pre
to .10/.12 posttest (sandwich \& no-pretest groups). Ozone .19 to .03/.02
(sandwich/no-pretest),  wrong
GHG  .24 to .07/.09 (sandwich/no-pretest). (test on total misconceptions, comparing sandwich
pretest to group-specific posttest)	&	.01	&	Fisher's exact test (two-sided)	\\
Participants don't mention energy leaving the earth until prompted.
Specifically, of the four codes that deal with this topic, only 6 mention
something about “trapped heat” in the pretest on the first (i.e., the only
unscaffolded) question.	&	.0002	&	Fisher's exact "two-sided"	\\
Use of infrared is greater posttest than pretest. Goes from 0 to 16 / 22 in
sandwich
/ no-pretest groups.	&	3.5E-08	&	Fisher's exact "two-sided"	\\
Sandwich: GHG Objective knowledge scores improve after the blurb	&	5.08E-05
&	$t(29) = -4.75$ (paired)	\\
No pretest: GHG Objective knowledge scores improve after the blurb	&	2.00E-06
&	$t(78.2) = -5.14$ (Welch)	\\
Sandwich: Light Objective knowledge scores improve after the blurb	&	3.94E-07
&	$t(29) = -6.51$ (paired)	\\
No pretest: Light Objective knowledge scores improve after the blurb	&	1.20E-04
&	$t(79.02) = -4.06$ (Welch)	\\
Sandwich: Energy Objective knowledge scores improve after the blurb	&	.04
&	$t(29) = -2.15$ (paired)	\\
No pretest: Energy Objective knowledge scores improve after the blurb	&
4.60E-04	&	$t(80.82) = -3.6547$ (Welch)	\\
Differences in mean GW attitudes are significant	&	.013	&	$t(72) = -2.28$
(paired / imputed)	\\
Sandwich pre to posttest: Increase in self rated knowledge is highly
significant	&	1.40E-05	&	$t(29) = 4.96$ (paired)	\\
No pretest: posttest (compared to sandwich pretest) increase in self rated knowledge
is significant	&	.014	&	$t(78.7) = 2.23$ (Welch)	\\
					
\end{longtabu}


\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

% Final \\ necessary to compile!
\caption{Summary of individual and group differences for Berkeley lecture room
    interventions. All results were \emph{a priori} unless the description starts
    with \emph{“post hoc.”}  \label{table:differences-classroom}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Individual and group differences in Berkeley lecture room interventions,
    continued}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Surprise is significantly greater in sandwich group than no-pretest group	&	.053	&
$t(42.08)=1.65$ (Welch)	\\
Posttest, slopes (i.e., correlations) between surprise and self-rated knowledge
differ between no-pretest group (negative, significant) and sandwich group (which was
numerically positive).	&	.036	&	$t(69)=2.137$ (interaction term in a
significant linear model)	\\
The no posttest group had a significantly higher word count than the sandwich
group's posttest answers for the first (objective) knowledge question.	& 2.5E-4 & $t(82.91) = -383$
(paired)	\\
No pretest (posttest): Females are significantly more accepting of climate change
than males	&	.048	&	$t(40.19) = -1.71$ (Welch)	\\
There is a significant positive correlation between number of times seeing An
Inconvenient Truth and GW attitudes	&	.022	&	$r(41) = .309$	\\

\end{longtabu}


\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

% Final \\ necessary to compile!
\caption{Summary of relationships between variables for Berkeley lecture room
    interventions. All results were \emph{a priori} unless description starts
    with \emph{“post hoc”}.  \label{table:relationships-classroom}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Relationships between variables in Berkeley lecture room interventions,
    continued}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Surprise is significantly positively correlated with change in total
objectively scored knowledge	&	.047	&	$r(28) = .355$	\\
\emph{Post hoc:} There is a significant correlation between self-rated knowledge
and GW attitudes on the pretest \emph{only} (differences in self-rated knowledge are
also insignificant). NB: we predicted the opposite result!	&	.012	&
$r(40) = .386$	\\
\emph{Post hoc:} Sandwich: Negative correlation between posttest self-rated
knowledge and \emph{change} in objective score	&	.011	&	$r(28) = -.458$	\\
\emph{Post hoc:} Sandwich: reversal in slope for the interaction term
between scored and self-rated knowledge pre to posttest	&	.047	&
$t(68) = -0.324$ (interaction term in a significant linear model)	\\
Self-ratings on carefulness in reading are significantly correlated with
posttest GW attitudes.	&	.035
&	$r(41) = .279$	\\
Rereading is significantly correlated with posttest GW attitudes.	&	.055
&	$r(41) = .247$	\\
\emph{Post hoc:} Counter to our initial hypothesis, there is a negative
correlation between rereading and posttest objective knowledge scores. NB: we
predicted the opposite result!	&	.019	&	$r(41) = -.356$	\\

\end{longtabu}


\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

% Final \\ necessary to compile!
\caption{Summary of “improvement” results for Brownsville lecture room
    interventions.  All results were \emph{a priori} unless the description starts
    with \emph{“post hoc.”}  \label{table:improvements-classroom}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Improvements in Brownsville lecture room interventions, continued}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Gains from pretest to posttest in mean GW attitudes are significant	&	1.30E-04
&	$t(38 = -4.02$ (paired / imputed)	\\
Use of “infrared” is greater posttest than pretest. Goes from 0 to 7/6 in
sandwich/no-pretest groups respectively (tested only for sandwich group)	&
8.00E-03	&	Fisher's exact "two-sided"	\\
Sandwich: pre to posttest: Increase in self rated knowledge is highly
significant	&	.001	&	$t(18) = 18$ (paired)	\\
Sandwich: GHG Objective knowledge scores improve after the blurb	&	.0034	&
$t(18) = 3.38$ (paired)	\\
No pretest: GHG Objective knowledge scores improve after the blurb	&
5.8E-4	&	$t(33.2) = 3.81$ (Welch)	\\
Sandwich: Light Objective knowledge scores improve after the blurb	&	.0095
&	$t(18) = 2.9$ (paired)	\\
No pretest: Light Objective knowledge scores improve after the blurb	&	1.4E-4
&	$t(25.6) = 4.48$ (Welch)	\\
Sandwich: Energy Objective knowledge scores improve after the blurb	&	.02
&	$t(18) = 2.5$ (paired)	\\
No pretest: Energy Objective knowledge scores improve after the blurb	&
2.9E-4	&	$t(36.8) = 4$ (Welch)	\\

\end{longtabu}

\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

\caption{Summary of failures to replicate and associated results with
    Brownsville lecture room
    interventions.  All results were \emph{a priori} unless the description starts
    with \emph{“post hoc.”}  \label{table:improvements-classroom}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Failures to replicate with Brownsville lecture room interventions,
    continued}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

No pretest: posttest increase in self-rated knowledge (compared to sandwich pretest) is
not significant	&	.51	&	$t(31.4) = -0.036$ (Welch)	\\
\emph{Post hoc:} Self-rated knowledge on  posttest is significantly lower for
no-pretest than sandwich group	&	.0019	&	$t(36.6) = -3.36$ (Welch)	\\
There is no correlation between self-rated knowledge and GW attitudes on the
pretest	&	.55	&	$r(17) = .15$	\\
Surprise is not significantly greater in sandwich group than no-pretest group	&	.18	&
$t(38.1) = 0.92$ (Welch)	\\

\end{longtabu}

\subsection{Discussion}

This experiment replicates and extends findings from prior interviews and
Cohen’s (\citeyear{cohen_san_2012_f}) survey, such that even rather well-educated
people initially held mostly non-normative understandings of global warming’s
mechanism. Only 400 words later, though (roughly the duration of a TV commercial
break), dramatic increases were observed in (1) mechanistic knowledge and (2)
global warming acceptance. Further, the increases were found in divergent U.S.
states and colleges. Certainly, this suggests that this educational intervention
is a reasonable object of study! Differences in surprise ratings between the
sandwich and no-pretest (“open-faced”) conditions further support the notion that
eliciting an explanation or theory prior to offering information increases
surprise and reduces post hoc rationalization and hindsight bias. (On surprise,
see Chapter~\ref{chap:two}; \cite{munnich_surprise_2007}.) 

% A graphical depiction
% of the probabilistic relationships we've established so far are in
% Figure~\ref{fig:causal-mechanism}.

% \begin{figure}
%     \begin{center}
%         \includegraphics{causal2.pdf}
%     \end{center}
%     \caption{A graphical model representing the relationship between forms of
%         psychological processing of factual information observed in
%         chapter~\ref{chap:mechanism}. Here, the relationship between
%         pre-existing information and surprise was the result of an experimental
%         manipulation and is assumed to be causal. NB: results in the Brownsville
%         data were numerically consistent with the significant result found with
%         the Berkeley data. Namely, there appears that activating pre-existing
%         knowledge boosts surprise.}
%     \label{fig:causal-mechanism}
% \end{figure}

In addition, we may note that there is scant difference between our sandwich and
no-pretest conditions in terms of posttest attitudes (across our two
populations, in one case the sandwich condition rates higher on posttest GW
attitudes, while in the other, the no-pretest condition rates higher). Thus, it
seems unlikely that a pretest incurs a greater burden of experimental demand
over the core intervention (400 words followed by a posttest). Moreover, in some
populations, the pretest may help to anchor self assessment (as with our UT
Brownsville data). And finally, the sandwich intervention appears to increase
reported feelings of surprise, and likely decreases post hoc bias
\parencite{rinne_estimation_2006}. Given these many benefits, in the work that
followed, we standardized on using a sandwich style intervention.

\section{Study 2: A Web-Based Intervention at UC Berkeley}
\label{sec:mech-uc-online}

Given the replicated demonstrations of significant attitude changes described
above, we proceeded to assess whether the mechanism-explanation effects we had
obtained were durable rather than transient. This study extended prior work by
employing a delayed followup test several days after our posttest. We also wondered
whether any ``experimental demand'' from the lecture room setting might have driven
our prior results, so we provided the intervention on-line; that is, we assessed
whether our materials would elicit significant attitude change even though
students participated using their own computers, without experimenter observation.
Thus we concurrently explored both  the longevity (with a delayed followup) and format
(online) aspects of our phenomenon. We also extended our prompts to incorporate
more demographic and introspection queries.

\subsection{Methods}
\label{sec:mech-online-methods}

\subsubsection{Participants}

Undergraduates ($N=80$) were recruited through the Research Participation
Program (RPP), administered by the University of California, Berkeley (UCB)
psychology department. For this study, I specifically recruited conservative
individuals and individuals with low GW acceptance based on data from the RPP
prescreening phase (completed 324 times during the RPP prescreening period,
though some students complete the form more than once---even though they get no
additional credit!). One participant was excluded due to technical problems with
his data.  (this was one one of the participants who took the RPP pretest).

Of the analyzed participants, 46 were female, 33 male. In addition, this study
reveals that (at least for Berkeley undergraduates), reported political party
affiliation is somewhat unstable! Of the 36 students who \emph{did} take the
RPP prescreening survey, we obtained two reports of political party. 12 of the
36 students reported something different before and after, though they did not
shift in a coherent way, as shown in Table~\ref{table:mech-rpp-party-shifts}.
The complete breakdown of stated party affiliation is given in
Table~\ref{table:mech-rpp-party-affiliations}.

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Tue May 21 15:56:02 2013
\begin{table}[ht]
\centering
\caption{Stated party affiliation for the 12 of 36 individuals for whom we have
    two responses for political party. It is difficult to discern a clear
    pattern here, beyond a general instability in stated party
    affiliation---though descriptively, there were more switched \emph{to} a
    party than \emph{away} from a party to a choice like “none” or
    “independent.” There are also more individuals claiming affiliation with
    conservative parties (Libertarian or Republican) than prior to the
    intervention. For example, an equal number of “none” responses shifted to
    “democrat” and “republican.” Similarly, three “democrats” shifted away, while
    four individuals shifted to become “democrats.” Notably, no individuals
    shifted to “green.”}
\label{table:mech-rpp-party-shifts}
\begin{tabular}{ll}
  \toprule
 RPP pretest & After Intervention \\ 
  \midrule
   democrat & libertarian \\ 
   democrat & none \\ 
   democrat & independent \\ 
   independent & democrat \\ 
   independent & libertarian \\ 
   none & democrat \\ 
   none & democrat \\ 
   none & republican \\ 
   none & republican \\ 
   none & independent \\ 
   decline to state & none \\ 
   decline to state & democrat \\ 
   \bottomrule
\end{tabular}
\end{table}

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Wed May 22 12:55:48 2013
\begin{table}[ht]
    \caption{Total number of individuals reporting affiliation with a given
        party. Note that we aggressively recruited conservatives to participate
        from the RPP prescreening population, yet still ended up with more
        republicans coming from the remainder of our population!}
    \label{table:mech-rpp-party-affiliations}
\centering
\begin{tabular}{rcc}
  \toprule
 & RPP pretest (fraction) & Study posttest (fraction) \\ 
  \midrule
  decline to state & 3.00 (.08) & 1.00 (.01) \\ 
  democrat & 12.00 (.33) & 26.00 (.33) \\ 
  green & 0 (0) & 1.00 (.01) \\ 
  independent & 2.00 (.06) & 3.00 (.04) \\ 
  libertarian & 1.00 (.03) & 4.00 (.05) \\ 
  none & 17.00 (.47) & 37.00 (.47) \\ 
  republican & 1.00 (.03) & 7.00 (.09) \\ 
  \midrule
  total & 36.00 (1.00) & 79.00 (1.00) \\ 
   \bottomrule
\end{tabular}
\end{table}

A final demographic measure of interest is conservativism. Given the large
number of students who completed the RPP pretest survey, we were able to
selectively contact those that we identified as the most conservative of the
300+ students who took the pretest. The RPP survey is standardized and shared
with all experimenters. In this case, the RPP survey includes 2 questions for
economic and social conservativism, while our survey only includes a question
for “conservativism.” A full treatment of the relationship between these
variables is not justified here. Briefly, social conservativism is quite
notably correlated with our later measure of non-specific conservativism ($r =
.84$; economic conservativism was correlated at $r = .68$). The addition of economic
conservativism as a second predictor or as a way to generate a mean
conservativism score on the pretest \emph{does} increase goodness of fit, but
the reductions in error are small. Moreover, the Bayesian Information Criterion
(BIC) weighs heavily in favor of a model using only social conservativism.
However we slice things, though, participants reported higher values for
non-specific conservativism after the intervention as compared to economic
\emph{or} social conservativism on the RPP pretest, as shown in
Table~\ref{table:mech-rpp-cons}. Thus, there is no unequivocal evidence here
that folks have moved in a conservative direction, but certainly they weren't
shifting in a more liberal direction! It does seem that recruitment of
conservatives may have been somewhat successful, based on the 0.5-point higher
score for participants who did the RPP pretest (and were thus the subject of
targeted conservative recruitment).

\begin{table}
    \caption{Conservativism scores from the RPP pretest and intervention
        posttest. Note that scores are not directly comparable, as the
        differing levels of specificity entailed somewhat different wording on
        the questions. That said, individuals were certainly close to the middle
        of the scale, and if anything moved closer to the middle on average}
    \label{table:mech-rpp-cons}
\centering
\begin{tabular}{rccc}
  \toprule
   & \multicolumn{3}{c}{Reported conservativism} \\
   & Social (pretest) & Economic (pretest) & Non-specific (posttest) \\
  \midrule
  Full sandwich & 3.86 & 3.25 & 4.31 \\
  No (RPP) pretest & & & 3.80 \\
  \bottomrule
\end{tabular}
\end{table}

\subsubsection{Materials and Procedure}

The survey and instructional materials were largely analogous to those reported
in Section~\ref{sec:mech-classroom}.  The primary difference was that
administration was conducted entirely online, using the Qualtrics Inc. (Provo,
UT) system, much as in Chapter~\ref{chap:prondi}. Eight items were added to pre
and posttest attitude surveys to add reliability to the related RTMD metrics
(specifically nationalism and religious affinities; these metrics will be
reported elsewhere). The \textsf{engage} item was added to provide a clearer
assessment of behavioral intentions (again, see
Appendix~\ref{app:survey-items}). Five further questions were introduced
immediately following the instructional material to elicit introspection (about
embarrassment, disagreement, etc.). These questions were distilled from the
result of guided interviews with participants in a pilot study that was quite
similar to this one, but carried out with participants using a computer in our
laboratory testing space.

RPP recruitment allowed us to administer a pretest to about half of the
undergraduates ($n=36$) between 8 and 26 days ($\mu=18.5$ days) before any of
the 80 students participated in the study, which may have allayed test-retest
effects (although we found little evidence for them in the experiments reported
above).  Thus, as with the above, some participants received the full survey
testing ``sandwich'' while for others we lacked the demographics and attitude
portion of the pretest. Note that this differs from previous “no-pretest”
groups, in that this group still provided their naïve description of the
mechanism of global warming. Thus, we will refer to our conditions as having
completed the  “full sandwich” or a “partial pretest.” Note that demographics
were \emph{also} collected following the primary intervention, because we did
not have them for all participants. Thus, for some participants, we collected
some demographic information twice (some of which is reported on below).
% Note - pretest here was ClarkSu2012_ClimateCognition_RPP_pre
A delayed posttest was given to all participants between 1 and 8 days later
($\mu=4$ days). The delay was not controlled, and as such, we should avoid
making inferences about the timecourse of retention over that delay. This
delayed test had the same format as the immediate posttest.  This range of
delays prior to this test was used to assess the timecourse of retention in
planning subsequent studies. We lack the power to test forgetting over time
here, but we did not observe any numerically.

\subsubsection{Analysis}

As we have now collected 3 observations of the same measure within subjects, for
many of the tests in this study, we shifted from using $t$-tests to using robust
mixed-effects regression. For this, we used the \textsf{lmer} function from the
\textsf{lme4} package in R, which handles a variety of unbalanced designs.
Goodness of fit for these regression models was assessed utilizing Type-II
sums of squares with the \textsf{Anova} function from the \textsf{car} package.
Once a null model was rejected, Tukey-style contrasts were computed as
simultaneous comparisons with \textsf{glht} from the \textsf{multcomp} package.
Unless otherwise stated, all tests were \emph{a priori}.

\subsection{Results} 
\label{sec:uc-online-results}

As before, a full report of statistical tests for this study is given at the end of this
section in Table~\ref{table:RPP-mech-results}. Statistics are provided in-text
when appropriate.
    
\subsubsection{Scored Knowledge: Learning the Global Warming Mechanism}

In general and as anticipated, we replicated results from Study 1
(Section~\ref{sec:mech-classroom}) and extended
them by finding that shifts were retained over the mean (four-day) delay.
Objectively scored knowledge was comparable to previously tested UC students,
rising from 3.8 on pretest to 6.5 posttest and 6.3 on delayed test (gains from
pretest were significant for both subsequent scores at $p<.0001$; drop from post
to delayed was not significant). These are plotted in
Figure~\ref{fig:RPP-mech-scored}. 

\begin{figure}
    \centering
    \includegraphics{RPP-mech-scored.pdf}
    \caption{Objectively scored knowledge before and after our intervention.
        Faint lines represent individual performance, while the bold line
        connects mean pre and posttest scores (indicated by a square and circle
        respectively). Mean delayed test score is indicated by a triangle at the
        mean time for taking the delayed test. A LOESS robust, smooth regression
        line is fit over the time period in which participants completed the
        delayed test.  Note that the pretest here was immediately prior to
        instruction (but shown at -24 hours). Participants overwhelmingly
        increased their scored knowledge following the intervention. While this
        study was not designed to assess forgetting in an individual over time
        (participants chose their own time to take the delayed test), it is
        interesting to note that earlier respondents tended to be scored lower
        than later respondents.  Most importantly, it is easy to spot
        individuals who improved or worsened over the delay.}
    \label{fig:RPP-mech-scored}
\end{figure}

\subsubsection{Self-Rated Knowledge}

\emph{Self-rated} knowledge means also increased markedly from pre to posttest (4.5
to 5.6 on a 9-point scale, $t(79)=8.5$, $p<.001$). Retention of this increase,
gratifyingly, was also noted on the delayed posttest (5.2, $t(79)=6.2$,
$p<.001$). The immediate increase in self-rated knowledge, replicates
results from the ``sandwich'' interventions in Study 1
(Section~\ref{sec:mech-classroom}). Scores are shown in
Figure~\ref{fig:RPP-mech-self}.

\begin{figure}
    \centering
    \includegraphics{RPP-mech-self.pdf}
    \caption{\emph{Self-rated} knowledge before and after our intervention. Note
        that the pretest here was immediately prior to instruction (but shown at
        -24 hours). Again, the bold line connects mean pre and posttest scores
        (indicated by a square and circle respectively). The mean delayed
        self-rating is indicated by a triangle at the mean time for taking the
        delayed test. A LOESS robust, smooth regression line is fit over the
        time period in which participants completed the delayed test.  As in
        Figure~\ref{fig:RPP-mech-scored}, there is a modest upward trend from
        early to late respondents, but recall that individuals chose their own
        time to respond to the delayed test.}
    \label{fig:RPP-mech-self}
\end{figure}

\subsubsection{Global Warming Acceptance Through Mechanistic Learning}

GW belief ratings (with higher ratings being more in concert with science’s
consensus) increased numerically from a 6.20 pretest mean to a 6.54 posttest
mean
% $t(79)=2.5$, $p=.006$
(a healthy improvement on our 1--9 Likert scales!). Some of this improvement
diminished over the following days, but most was retained: the mean score on the
delayed posttest was 6.44.  %($t(79)=1.7$, $p=.05$).  These results are
significant using a naïve imputation approach. A more recent analysis, however,
implies that these results may not be so clear.  In particular, using the
\textsf{lmer} package yielded insignificant improvements even from pre to
posttest. Upon closer inspection, the breakdown for GW attitudes is quite
different across full-sandwich and partial-pretest conditions, as seen in
Table~\ref{table:RPP-mech-gw-breakdown}. Unlike in previous studies, the
full-sandwich conditions (i.e., the condition with GW attitudes available from the
RPP pretest) contained a good number of participants who were recruited for
their conservativism (and there is a clear condition difference on this measure).
Thus, we are not as justified in assuming the pretest is representative of
\emph{all} participants. In the terminology described by
\textcite{fox_applied_2008}, these data are not \emph{missing at random} (MAR).
Thus, we cannot apply the same sort of simple imputation to obtain a valid
result.  Individual scores are depicted in Figure~\ref{fig:RPP-mech-GW}. While
we may be able to salvage this data using more sensitive techniques, such
efforts are unwarranted given the robust successes reported in
Sections~\ref{sec:mech-classroom} and \ref{sec:mech-mturk}, and the agreement
here with the pattern obtained there.

% BUT we have over 300 folks from RPP. So, we should be able to address this at
% least partially that way. Throw everyone into one big model?


% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Fri May 24 16:15:32 2013
\begin{table}[ht]
\centering
\caption{Mean GW ratings for full-sandwich and partial-pretest conditions. Note that the
    increase from pretest to posttest is only 0.16 for individuals in the
    sandwich group, and delayed test results are lower than where they started.
    The partial-pretest group, however, starts much higher and stays higher.}
\label{table:RPP-mech-gw-breakdown}
\begin{tabular}{rccc}
  \toprule
        & Pretest & Posttest & Delayed test \\ 
  \midrule
  Partial (no RPP) pretest & NA & 6.68 & 6.65 \\ 
  Full sandwich & 6.20 & 6.36 & 6.18 \\ 
   \bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics{RPP-mech-GW.pdf}
    \caption{Reported global warming (GW) beliefs and attitudes before and after
        our intervention. Note that the pretest here was administered on average
        18.5 days prior to instruction as a part of UC Berkeley's undergraduate
        participant pool (RPP) prescreening (but is shown at -24 hours). Once
        again, the bold line connects mean pre and posttest scores (indicated by
        a square and circle respectively). The mean delayed self-rating is
        indicated by a triangle at the mean time for taking the delayed test. A
        LOESS robust, smooth regression line is fit over the time period in
        which participants completed the delayed test.  We can also notice here that all
        of the individuals who yielded the lowest ratings for normative beliefs
        and attitudes took the delayed test quite early. But given
        self-selection of the time for the delayed test, we should certainly be
        taking those initial dips with a grain of salt. We cannot assume that
        the figure represents an actual retention timecourse!}
    \label{fig:RPP-mech-GW}
\end{figure}


\subsubsection{Surprise}

As explained in the Methods, some participants received a full “sandwich”
intervention, including beliefs and attitudes administered during RPP
prescreening. All participants, however, completed a knowledge pretest. Thus,
we were able to informally compare these two partially novel conditions with our
previous profiles for reported surprise (i.e., with
Figure~\ref{fig:uc-mech-surprise}). The surprise results from our current
conditions are provided in Figure~\ref{fig:rpp-mech-surprise}, and provide
further support for the centrality of the initial \emph{knowledge} pretest.

\begin{figure}
    \centering
    \includegraphics{RPP-mech-surprise-by-group.pdf}
    \caption{Surprise ratings for individuals in Study 2. The shape of both of
        these distributions much more closely approximate that of the sandwich
        participants in Study 1 (See, e.g., Figure~\ref{fig:uc-mech-surprise}).
        In particular, both conditions here yield surprise values in the 7+
        range.  Thus, it seems to matter less if or when participants receive
        the beliefs and attitudes surveys---the higher surprise values appear
        to be linked to asking participants to “put their cards on the table”
        with a knowledge pretest.}
    \label{fig:rpp-mech-surprise}
\end{figure}

In addition to repeating the surprise question from Study 1, on the basis of
informal interviews, we had included another related question asking if people
were “surprised or embarrassed about their own lack of knowledge.” This item
does seem to have elicited somewhat higher scores, with a mean of 3.6 vs. 2.9
for the original surprise question. Interestingly, this not only appears to (at
least partially) alleviate a floor effect obtained with the straight surprise
question, but it also demonstrates a reversal of the ranking of affective
experiences as compared to the study with effective, representative numbers in
Section~\ref{sec:CCO-ndi-participants}. The distributions for these responses
are shown in Figure~\ref{fig:rpp-mech-embarrass}.

\begin{figure}
    \centering
    \includegraphics{RPP-mech-embarrass-by-group.pdf}
    \caption{Ratings of “embarrassment or surprise at their own lack of
        knowledge” for individuals in Study 2. Distributions are again similar in
        notable ways between groups. In particular, there appears to be less of
        a floor effect here than we obtain with our original phrasing of
        surprise (consistent with a higher mean).  Individuals are again using
        most of the range, including ratings in the 7+ range.}
    \label{fig:rpp-mech-embarrass}
\end{figure}

\subsubsection{Micro-analysis of GW ratings}

Table~\ref{table:uc-online-gw-means} reports the mean rating across participants
for agreement with individual items. (For the full text of these items, see
Appendix~\ref{app:survey-items}.) The largest gains were found in agreeing with
item \textsf{gw1_2}: ``Human activities are largely responsible for the climate
changes\ldots'' (a 0.25 gain) and certainty that global warming is occurring (a
0.19 gain).  In general, gains were fairly consistent across all GW measures,
ranging only down to 0.08 at the lowest (the relatively broad, accusatory item
\textsf{gw2_4}: “humans are severely abusing the environment”). Interestingly
importance of lifestyle changed the most (0.27, though this was not included in
the tested average GW variable).  Expectation of engagement, dishearteningly,
clocks in at a 0.05 \emph{drop}! 
% Enter motivational interventions like Oroeco?

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Thu May  9 12:43:59 2013
\begin{table}
\caption{Mean GW ratings, online with UC Berkeley undergrads. For means, please refer to
         Table~\ref{table:RPP-mech-gw-breakdown}} 
\label{table:uc-online-gw-means}
\centering
\begin{tabular}{>{\sffamily}rccc}
  \toprule
 & Pretest & Posttest & Delayed test \\ 
  \midrule
  gw1_2 & 6.61 & 6.86 & 6.36 \\ 
  gw2_1 & 5.19 & 5.31 & 5.25 \\ 
  gw2_2 & 6.61 & 6.81 & 6.67 \\ 
  gw2_3 & 5.81 & 5.97 & 5.97 \\ 
  gw2_4 & 6.78 & 6.86 & 6.67 \\ 
  engage & 5.91 & 5.86 & 6.11 \\ 
  lifesty & 4.83 & 5.11 & 4.94 \\ 
  \bottomrule
\end{tabular}
\end{table}

\subsubsection{Correlations}

Scored knowledge and self-rated knowledge are significantly correlated pretest,
so participants have reasonable meta-cognition here. However, unlike with our
Berkeley students in Study 1, there was only a not quite marginally significant
(and smaller) correlation between na\"ive pretest self-rated knowledge and GW
attitudes ($r(34)=.27$, $p=.11$). Thus, while the validity of self-rated
knowledge (as a predictor of actual knowledge) appears robust across Berkeley
student populations, the relationship between self-rated knowledge and GW
attitudes is weak, or perhaps even spurious.

\subsection{Discussion}

In sum, this study extends the finding that well-considered information, even
received online, increases GW acceptance and behaviorally relevant attitudes;
the conceptual changes that result from reading even 400 words have notable
longevity. As we'll see below, these effects have been replicated with members
of the general public as well. Computer-based interventions often scale well,
enhance reliability, and prove cost-effective; given our results, we recommend
the online distribution of mechanistic explanations, especially about climate
change. A collection of videos to this effect is currently online at
\url{http://HowGlobalWarmingWorks.org}.

Delayed followup tests occurred over a range of delays, and retention here was
used in determining intervals for future studies.  Given the almost total lack
of mean forgetting over the observed interval, we start our subsequent
study's delayed test after a longer delay (specifically, for a one week period
starting 4 days after the initial intervention).

\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

\caption{Summary of results from Study 2.\label{table:RPP-mech-results}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Results from Study 2, continued.}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Scored knowledge is different across the three testing times in a repeated-measures ANOVA. &
2.2e-16 & $\chi^2(2)=128.39$  \\
Scored knowledge is higher in the posttest than in the pretest. & 5e-3 & $z=10.09$ \\
Scored knowledge is higher in the delayed test than in the pretest. & 5e-3 & $z=9.52$ \\
Scored knowledge is not significantly lower in the delayed test than in the posttest. &
.813 & $z=-0.612$ \\
Self-rated knowledge is different across the three testing times in a repeated-measures ANOVA. &
2.2e-16 & $\chi^2(2)=110.25$ \\
Self-rated knowledge is higher in the posttest than in the pretest. & 1e-5 & $z=10.47$ \\
Self-rated knowledge is higher in the delayed test than in the pretest. & 1e-5 &
$z=5.922$ \\
Self-rated knowledge is lower in the delayed test than in the posttest. &
1.5e-5 & $z=-4.55$ \\
We are unable to reject the null hypothesis that GW attitudes are the same
across the three testing times given this dataset (but note that the pattern is
still consistent with Studies 1 and 3). & 0.21 & $\chi^2(2) = 3.09$ \\
\emph{Self-rated} knowledge is significantly correlated with scored knowledge on the
pretest. & 4.7e-6 & $r(76)=.49$ \\
\emph{Self-rated} knowledge is \emph{not} quite marginally correlated with GW attitudes on
the pretest. & .11 & $r(34)=.27$ \\

\end{longtabu}

\section{Study 3: An Intervention with Amazon Mechanical Turk}
\label{sec:mech-mturk}

\subsection{Methods}

Experimental methods in this study were nearly identical to Study 2, above
(detailed in Section~\ref{sec:mech-online-methods}). The primary difference here
was that participants were recruited through Amazon's Mechanical Turk (discussed
in more detail in Section~\ref{sec:mturk-problems}. In general, the remainder of the
methods focuses only on the differences with those reported in
Section~\ref{sec:mech-online-methods}.

\subsubsection{Participants}

“Workers” on the Amazon Mechanical Turk platform ($N=41$) were recruited to
complete a two-part survey. Language used in recruitment made no mention of
climate change, and was titled “Politics, Science, and Your Attitudes.”
Approximately 75\% of these individuals ($n=30$) completed the delayed test.
After removal of problematic participants (as described in the “data quality”
section below), we were left with 38 participants in the primary intervention,
with 28 in the delayed test. 17 of the 38 retained participants were female.
Two of our participants reported as being born outside the United States, but
residing here for at least 22 years. Stated party affiliations are listed in
Table~\ref{table:cco-mech-party}. Mean conservativism was 3.9 out of 9, which is
comparable to our college students above. Participants, however, were far more
likely to declare being a Democrat or Republican than our college students above.
While the sample is still clearly biased towards the Democratic/liberal end of
the political spectrum, the ratio between Republicans and Democrats is far less
extreme than in previous samples. 

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Sun Jun  2 14:31:37 2013
\begin{table}[ht]
    \caption{Stated party affiliations for participants in Study 3 (through
        Mechanical Turk).}
    \label{table:cco-mech-party}
\centering
\begin{tabular}{rcc}
  \toprule
 Party & Number (percentage) \\ 
  \midrule
  democrat     & 22 (58\%) \\ 
  republican   & 8 (21\%) \\ 
  independent  & 6 (16\%) \\ 
  none         & 1 (3\%) \\ 
  other        & 1 (3\%) \\ 
  \midrule
  total        & 38.00 (100\%) \\ 
   \bottomrule
\end{tabular}
\end{table}

% This was a spurious result that resulted from a change in wording between our
% demographics and RPP demographics
% A final interesting feature of our population is that, as in
% Section~\ref{sec:CCO-ndi-participants}, all participants reporting
% their faith as Christian self-identified as Catholic. That is, we had no
% “protestant” participants.

\subsubsection{Materials and Procedure}

On the basis of our previous two studies, the most convenient design that still
elicits the highest surprise was pretest immediately before the mechanism
explanation. Moreover, this pretest can include all measures---eliciting descriptions
prior to instruction maximized surprise, and the timing of other attitudinal
measures seemed to have little effect (or may have actually reduced our power to
observe shifts in attitudes). Thus, here, all participants completed a pretest
immediately prior to the main intervention and posttest, most closely
approximating the sandwich intervention from Study 1 with the addition of a
delayed test some days later.

The most interesting difference in procedure between this and Study 2 is the
utilization of Mechanical Turk. A minor change is that the retention interval
was extended to begin 4 days later (with participants choosing when to take the
delayed posttest), with the longest interval being 10.8 days
($\mu=5.5$ days). A reminder was sent through the Mechanical Turk bonus system,
by which participants were paid 5 cents and given a message that the delayed test
was open for completion. The majority of payment was provided at the time of
completion of the delayed test. Perhaps unsurprisingly, most participants
responded shortly after the first reminder, and the majority of the remainder
responded shortly after the final reminder.

Materials were identical, apart from the addition of a question requesting the
individual's “worker ID”---a unique identifier used to track and pay
individuals, akin to a social security number assigned by Amazon.

\subsubsection{Data quality on Amazon Mechanical Turk}

As discussed in Section~\ref{sec:mturk-problems}, the use of an anonymous,
on-line labor pool raises a number of additional concerns about data quality.
Here, we apply the same approach to IP-based testing for participant honesty
about their location, and checking against re-takes.  An additional concern
arises in this study---somewhat bizarrely, as this does not reduce time
required, or increase payment---participants may copy and paste from online
sources.

Again, Amazon attempted to prevent re-takes as requested, and no duplicate IP
addresses were detected. Two participants, however, lacked an IP address.
Manual inspection revealed nothing anomalous about these participants, however,
so they were retained.

The Mechanical Turk system was again set to allow only individuals in the U.S.
as participants. Verification of location was again obtained using GeoLite data
created by MaxMind (available from \url{http://www.maxmind.com}).  An additional
check was available given IP addresses captured during both the primary
intervention as well as the delayed test. On the primary survey, participants
indicated the state they were in.  Participant IP addresses from both
intervention and delayed followup surveys were subsequently checked against this
reported location. Here, two individuals IP’s appeared to be located in India
and Turkey, and thus these participants were excluded. Most other participants
had IP addresses that resolved to the state they claimed to be from. One
participant’s IP address was not listed in the MaxMind database, and was traced
to Hughes Net, a U.S.-only satellite internet provider.

Checking for plagiarism is a well-trod topic in academia today, though there is
no one clear approach. For the purposes of this survey, we relied on a
combination of coder judgement, and automated checking using Google’s Custom
Search API. A complete description of the process I used is provided in
Appendix~\ref{app:detecting-plagiarism}. In summary, I used a combination of my
own and others' judgement to identify clear cases of plagiarism. I then
developed an approach using Google search to identify texts that already existed
on the internet, being sure that this method caught the clear cases we’d
identified using manual inspection. Ultimately, two individuals clearly 
copied and pasted materials from the internet. These could have both been
identified by the unusual presence of extended unicode characters in the text of
their answers, as well as the presence of newlines.

Note that one individual was \emph{both} foreign \emph{and} copied text directly
form the web. Thus, we exclude only 3 subjects total.

While survey consistency seemed reasonable for all remaining participants, it
did highlight two individuals who answered almost exclusively 1 or 9 to all
items. These individuals were retained in the analyses below, as they still
provided information on their beliefs. In particular, their responses were
consistent on reverse-coded catch-trials, and their textual descriptions were
consistent with taking the task seriously.

% I did the survey consistency checks in another notebook, and don’t think it’s
% worth reporting on that here. All Ss were retained. I also checked using
% excel’s conditional formatting that no participants simply answered the same
% thing all the way through. 

% Analysis - exactly the same as before. Doesn't seem to warrant inclusion, but
% it might be nice to have the link to the section above.

\subsection{Results}

Overall, we replicate our central results from Studies 1 and 2 above. As before,
full statistics are reported in Table~\ref{table:CCO-mech-results} at the end of
this section. Only information directly supporting the narrative is included in
the textual exposition.

\subsubsection{Problems and Data Quality}

While the text of the delayed test was like the pretest (i.e., the first
question made no affordance for “if you would add anything”) three individuals
still put “nothing to add” for all three questions. These individuals were
retained as, if anything, they should weaken retention effects after a delay.
Moreover, they all still answered the Likert survey questions. While not
definitive, this is certainly one clue that individuals recruited through Mechanical
Turk are perhaps rushing a bit more than our previous college samples. This may
also be the reason we observed two individuals who only provided 1's and 9's
above.
% TODO give precise number

\subsubsection{Scored Knowledge: Learning and Global Warming Mechanism}

Again, we replicated results from Studies 1 and 2 by finding that shifts were
retained over the mean 5.5-day delay. Scored knowledge was comparable to
previously tested students, rising from 1.9 on pretest to 4.8 posttest and 3.9
on delayed test (on a 0--9 scale; gains from pretest were significant at
$p<.002$ for both subsequent scores). These are plotted in
Figure~\ref{fig:CCO-mech-scored}. 

\begin{figure}
    \centering
    \includegraphics{CCO-mech-scored.pdf}
    \caption{Objectively scored knowledge before and after our intervention.
        Faint lines represent individual performance, while the bold line
        connects mean pre and posttest scores (indicated by a square and circle
        respectively). Mean delayed test score is indicated by a triangle at the
        mean time for taking the delayed test. A LOESS robust, smooth regression
        line is fit over the time period in which participants completed the
        delayed test.  Note that the pretest here was immediately prior to
        instruction (but shown at -24 hours). Participants overwhelmingly
        increased their scored knowledge following the intervention. As before,
        this study was not designed to assess forgetting in an individual over
        time, but it is interesting to note that, cross-sectionally, we again
        obtain relatively high scores later in our delayed testing. In contrast
        to Study 2 in Section~\ref{sec:mech-uc-online}, most participants took
        the study the day they received their reminder, as is reflected by the
        mean delay being closer to the left edge of the LOESS regression line.}
    \label{fig:CCO-mech-scored}
\end{figure}

\subsubsection{\emph{Self-rated} Knowledge}

Self-rated knowledge means also increased markedly from pre to posttest (4.2
to 4.7 on a 1--9 scale, $p<.01$). Retention of this increase, however,
failed to exceed our significance threshold (4.6, $p=.11$). These scores are
reported in Figure~\ref{fig:CCO-mech-self}. It seems not unlikely, given our
previous results, that more subjects would yield a significant difference here.
Even so, it should be noted that studies with college students tended to yield
increases of over a point on self-rated knowledge (at least with “sandwich”
interventions such as this one). Thus, even our significant pre to posttest
gains are small compared to the UCB population.

\begin{figure}
    \centering
    \includegraphics{CCO-mech-self.pdf}
    \caption{\emph{Self-rated} knowledge before and after our intervention. Again,
        faint lines represent individual performance, while the bold line
        connects mean pre and posttest scores (indicated by a square and circle
        respectively).  Mean delayed test score is indicated by a triangle at
        the mean time for taking the delayed test. A LOESS robust, smooth
        regression line is fit over the time period in which participants
        completed the delayed test.  Note that the pretest here was immediately
        prior to instruction (but shown at -24 hours). Participants
        overwhelmingly increased their scored knowledge following the
        intervention. As before, this study was not designed to assess
        forgetting in an individual over time, but it is interesting to note
        that we again obtain relatively high scores later in our delayed
        testing. In contrast to Study 2 in Section~\ref{sec:mech-uc-online},
        most participants took the study the day they received their reminder,
        as is reflected by the mean delay being closer to the left edge of the
        LOESS regression line.}
    \label{fig:CCO-mech-self}
\end{figure}

\subsubsection{Global Warming Acceptance Through Mechanistic Learning}

Happily, while introspection regarding knowledge seemed to yield somewhat
smaller gains than in Studies 1 and 2, GW belief ratings increased significantly
from a 6.34 pretest mean to a 6.64 posttest mean ($p=.001$). Some of this
improvement diminished over the following days, but most was retained: the mean
score on the delayed posttest was 6.58 ($p=.006$). Note that these values are
very much in line with those obtained in the previous studies. Individual
attitude breakdowns are reported in Table~\ref{table:CCO-mech-gw-means}.
Individual data are depicted in Figure~\ref{fig:CCO-mech-GW}.

% latex table generated in R 2.15.1 by xtable 1.7-1 package
% Fri May 31 18:42:37 2013
\begin{table}
\caption{Mean GW ratings for the Mechanical Turk mechanism intervention. Note
    that means are only computed over the items with codes starting with
    “\textsf{gw}.” Item codes are explained in Table~\ref{table:rtmd-questions}
    in the Appendix.}
\label{table:CCO-mech-gw-means}
\centering
\begin{tabular}{>{\sffamily}rccc}
  \toprule
         & Pretest & Posttest & Delayed test \\ 
  \midrule
  gw1_2 & 6.87 & 7.34 & 7.07 \\ 
  gw2_1 & 6.03 & 6.55 & 6.25 \\ 
  gw2_2 & 6.82 & 7.05 & 7.04 \\ 
  gw2_3 & 6.05 & 6.42 & 6.36 \\ 
  gw2_4 & 6.89 & 7.05 & 6.86 \\ 
  engage & 6.37 & 6.42 & 6.46 \\ 
  lifsty & 5.37 & 5.63 & 6.04 \\ 
  \midrule
  \textrm{mean} & 6.34 & 6.64 & 6.58 \\
   \bottomrule
\end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics{CCO-mech-GW.pdf}
    \caption{Reported global warming (GW) beliefs and attitudes before and after
        our intervention on Mechanical Turk. A significant increase in
        participant ratings is observed after the intervention, and this gain is
        significantly retained (compared to pretest) in the delayed test. Once
        again, faint lines represent individual ratings, while the bold line
        connects mean pre and posttest scores (indicated by a square and circle
        respectively).  Mean delayed test score is indicated by a triangle at
        the mean time for taking the delayed test. A LOESS robust, smooth
        regression line is fit over the time period in which participants
        completed the delayed test. The pretest here was immediately prior to
        instruction (but shown at -24 hours).}
    \label{fig:CCO-mech-GW}
\end{figure}


\subsubsection{Surprise}

As in Study 2 above, individuals ranked their “embarrassment or surprise at their own
lack of knowledge” higher than straight surprise. Mean values were 2.9 and 4.1
respectively for straight surprise and surprise/embarrassment, quite comparable
to Study 2 in this chapter, and again a reversal from the Study 2 in
Section~\ref{sec:CCO-ndi-participants}. Distributions for both questions are depicted in
Figure~\ref{fig:CCO-mech-surprise}). 

\begin{figure}
    \centering
    \includegraphics{CCO-mech-surprise-by-question.pdf}
    \caption{Surprise and “embarrassment or surprise at their own lack of
        knowledge” ratings for individuals in Study 3.  Embarrassment appears to
        suffer less of a floor effect than surprise.}
    \label{fig:CCO-mech-surprise}
\end{figure}

% TODO need to put a basic record of folks who claim they knew before, but
% forgot

\subsubsection{Factors with no observed effect}

As with our UC students, participants demonstrated a significant correlation
between self-rated knowledge and actual knowledge ($r(36)=.49$).  And, as in
Study 2, we again failed to replicate a significant relationship between
self-rated knowledge and GW attitudes on the pretest. Given these repeated
failures to replicate, it seems prudent to abandon the naïve relationship
between self-rated knowledge and GW attitudes (at least in this general sense).

% TODO
% Similarly, with this sample, we are able to examine the effects of
% conservativism or political party. Figure XXX shows attitudes broken out by
% Republican, Democrat (and Other).

\subsection{Discussion}

This study provides an evaluation of our Global Warming mechanism intervention
in a much more true-to-life scenario. Specifically, we should be concerned with
evaluating a population that is representative of individuals that might engage
with our materials were they to be made generally available on line. The
participants we obtained here on Mechanical Turk are likely much closer to this
ideal than our college students above.

\begin{longtabu}{X[2.5]S[table-parse-only]X[l]}

\caption{Summary of results from Study 3.\label{table:CCO-mech-results}}\\ 
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endfirsthead

% The empty option prevents a TOC entry from being generated
\caption[]{Results from Study 3, continued.}\\
\toprule
Result & {$p$-value} & Statistic \\ \midrule
\endhead

\bottomrule
\endfoot

Scored knowledge is different across tests in a repeated-measures ANOVA. &
2.7e-8 & $\chi^2(2)=34.87$  \\
% Divided by two, a priori one-tailed
Scored knowledge is higher in posttest than in pretest. & 
5e-5 & $z=-5.86$ \\
Scored knowledge is higher in delayed test than in pretest. & 
.0014 & $z=3.30$ \\
Self-rated knowledge is different across tests in a repeated-measures ANOVA. &
.021 & $\chi^2(2)=7.69$ \\
Self-rated knowledge is higher in posttest than in pretest. & 
.0088 & $z=2.72$ \\
Self-rated knowledge is \emph{not} quite marginally higher in the delayed test than in
pretest. & 
.11 & $z=1.69$ \\
Mean GW attitude is different across tests in a repeated-measures Anova. &
.00091 & $\chi^2(2)=14.01$ \\
Mean GW attitude is higher in posttest than in pretest. & 
.00085 & $z=3.445$ \\
Mean GW attitude is higher in delayed test than in pretest. & 
.0062 & $z=2.84$ \\
Self-rated knowledge is significantly correlated with scored knowledge at
pretest. & 
.0017 & $r(36)=.49$ \\
Self-rated knowledge is \emph{not} significantly correlated with GW attitudes at
pretest. & 
.32 & $r(36)=.16$ \\

\end{longtabu}

\section{Summary and Conclusions}

We've shown across a number of populations that ignorance of the basic
physical/chemical mechanism of the greenhouse effect is nearly universal. In
addition to this research, \textcite{felipe_numerical_2012} describes successes
with a curriculum involving the mechanism with 11th graders (and see
Chapter~\ref{chap:prondi} for more on the numerical estimation aspects of that
study). Across a variety of intervention styles, we have shown that individuals
are able to markedly increase their ability to describe the greenhouse effect,
and that such an intervention additionally shifts climate-related beliefs and
attitudes.

% This could perhaps go into the final conclusions?

As can be seen from the work described above, the act of educating the American
public about the basic mechanism of climate change is a daunting, multi-faceted
challenge. And even this is not sufficient to know if such an endeavor will
truly effect some positive action towards the larger problem of climate change.
For that, we will need to examine some connection to behavior. This is the
clearest lack in the current research. Such an endeavor will be even more
challenging than the above, but it is of critical importance! 

Overall, however, we have seen evidence that materials such as those exhibited
in Appendix~\ref{app:400words} are likely to be effective both in college
lecture room as well as online. Evidence from related studies has provided
additional support for effective application in high school classrooms.

%TODO - things we can really address with the followup data:

% Look at the relationship between folks saying they learned / were surprised /
% stuff was knew on the basis of their scored knowledge. This is an argument
% against the notion that folks knew things already, but didn’t think to mention
% them given our prompts.

\section*{Acknowledgements}

The work reported in this chapter has been previously published, in part, in
\textcite{ranney_changing_2012,ranney_improving_2012_f}, and
\textcite{clark_knowledge_inpress}.  All such material is re-used here with the
permission of my co-authors, the publishers, and the Graduate Division at the
University of California, Berkeley.
